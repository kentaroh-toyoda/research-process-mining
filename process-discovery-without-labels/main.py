import scipy as sp
import numpy as np
import pandas as pd
import os
import sys
import copy
import time
import re
import random
from string import ascii_uppercase
from datetime import datetime 
import pickle
from pathlib import Path
import dateutil.parser
# for bruteforce
from SetSimilaritySearch import all_pairs
from itertools import permutations
# for CV
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import GroupKFold
# optimizer
import concurrent.futures
from evolutionary_operators import cxUniform, mutateGaussian, binary_tournament_selection
from leea import CEA_MIP

import pm4py
from pm4py.objects.conversion.log import converter as log_converter
#  from pm4py.objects.log.importer.xes import importer as xes_importer
# miners
from pm4py.algo.discovery.inductive import algorithm as inductive_miner
from pm4py.algo.discovery.heuristics import algorithm as heuristics_miner
# performance metrics
from pm4py.algo.evaluation.replay_fitness import algorithm as replay_fitness_evaluator
from pm4py.algo.evaluation.precision import algorithm as precision_evaluator
from pm4py.algo.evaluation.generalization import algorithm as generalization_evaluator
from pm4py.algo.evaluation.simplicity import algorithm as simplicity_evaluator

import argparse
parser = argparse.ArgumentParser(description="Label-free process discovery via optimization")
parser.add_argument('--miner', dest='miner', type=str, help='a process discovery algorithm to be used')
parser.add_argument('--method', dest='method', default='GA', type=str, help='a method to be tested')
parser.add_argument('--round', dest='round', type=int, help='number of iterations')
parser.add_argument('--dataset', dest='dataset', default='../datasets/BPIC12.csv', type=str, help='an event log (csv) to be evaluated')
parser.add_argument('--results_path', dest='results_path', default='./results', type=str, help='the path where results will be stored (default: ./results)')
parser.add_argument('--pop', dest='pop', default=20, type=int, help='number of populations to be generated by an optimizer (default: 20)')
parser.add_argument('--gen', dest='gen', default=100, type=int, help='number of generations to be generated by an optimizer (default: 100)')
parser.add_argument('--metric', dest='metric', default='recall', type=str, help='metric function for score (default: recall)')
parser.add_argument('--n_max_traces', dest='n_max_traces', default=100, type=int, help='number of traces for testing (default: 0.1)')
parser.add_argument('--n_max_activities', dest='n_max_activities', default=10, type=int, help='n_max_activities (default: 10)')
parser.add_argument('--evaluation_method', dest='evaluation_method', default='CV', type=str, help='evaluation_method (default: CV)')
parser.add_argument('--min_cosine_sim', dest='min_cosine_sim', default=0.5, type=float, help='min_cosine_sim (default: 0.5)')
args = parser.parse_args()

miner = args.miner
method = args.method
dataset = args.dataset
results_path = args.results_path
Path(results_path).mkdir(parents=True, exist_ok=True)
pop = args.pop
gen = args.gen
metric = args.metric
n_max_traces = args.n_max_traces
n_max_activities = args.n_max_activities
evaluation_method = args.evaluation_method
min_cosine_sim = args.min_cosine_sim

# read a csv log file
original_log_df = pd.read_csv(dataset)
# replace NaN with '' so that an evaluation function can work
original_log_df = original_log_df.fillna(np.nan).replace([np.nan], [''])

# dim is the number of columns
dim = original_log_df.columns.size
print('number of columns:', dim)

# keep the indices of case_id, activity, and timestamp for calculating accuracy
h = original_log_df.columns.values.tolist()
labels = [
        h.index('case:concept:name'),
        h.index('concept:name'),
        h.index('time:timestamp')
        ]

print('correct label indices:', labels)

def f(x, model_info=None):
    # NOTE: Pandas' and Python's deepcopy does not actually copy an object, and we thus need to initialize table header against the original log df every time
    # initialize table header with alphabets
    original_log_df.columns = [ascii_uppercase[i] for i in range(0, dim)]
    # copy from the original
    event_log_df = original_log_df.copy()

    if method == 'GA':
        # random keys for sequencing and optimization
        indices = np.argsort(x)[0:3]
        print('suggested label indices:', indices.tolist())
    elif method == 'bruteforce':
        indices = x
        print('suggested label indices:', list(indices))

    # assign case_id ('case:concept:name'), activity ('concept:name'), and timestamp ('time:timestamp')
    event_log_df.columns.values[indices[0]] = 'case:concept:name'
    event_log_df.columns.values[indices[1]] = 'concept:name'
    event_log_df.columns.values[indices[2]] = 'time:timestamp'

    # preprocessing
    # sample dataset by case_id. We keep sampling_ratio of case_ids
    # list case_ids
    case_ids = set(event_log_df['case:concept:name'])
    print('number of traces:', len(case_ids))
    if len(case_ids) > n_max_traces:
        # determine which case_ids will be kept
        sampled_indices = random.sample(case_ids, n_max_traces)
        # filter out the others
        event_log_df = event_log_df[event_log_df['case:concept:name'].isin(sampled_indices)]
    case_ids = set(event_log_df['case:concept:name'])
    print('number of traces (after sampling):', len(case_ids))

    # when traces have many activities, we just use first n_max_activities in it.
    event_log_df = event_log_df.groupby('case:concept:name').head(n=n_max_activities)

    # do pretest
    if not pretest(event_log_df):
        return 0


    if evaluation_method == 'no split':
        # split a dataset into training and test
        print('dataset size', len(event_log_df.index))

        # dataset need be sorted by timestamp (https://pm4py.fit.fraunhofer.de/documentation#item-import-csv)
        event_log_df = event_log_df.sort_values('time:timestamp')

        # convert a df to an event log
        event_log = log_converter.apply(event_log_df, variant=log_converter.Variants.TO_EVENT_LOG)

        print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'event log converted')
        print(event_log_df[['case:concept:name', 'time:timestamp', 'concept:name']].head())

        net, im, fm = discover_model(event_log, miner)
        print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'process model discovered')
        score = evaluate_score(event_log, net, im, fm, metric)
        print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'score evaluated:', score)
        return score

    if evaluation_method == 'CV':
        # assume 3-fold CV
        n_splits = 3
        # n_repeats = 10
        # split the dataset
        # ref. https://scikit-learn.org/stable/modules/cross_validation.html
        gkf = GroupKFold(n_splits=n_splits)
        X = list(event_log_df.index)
        groups = list(event_log_df['case:concept:name'])
        scores = [cross_validation(event_log_df, train, test) for train, test in gkf.split(X, groups=groups)]
        score = np.mean(scores)
        print('final score:', score)
        return score

def cross_validation(event_log_df, train, test):
    # dataset need be sorted by timestamp (https://pm4py.fit.fraunhofer.de/documentation#item-import-csv)
    #  print("%s %s" % (train, test))
    train_log_df = event_log_df.iloc[train]
    train_log_df = train_log_df.sort_values('time:timestamp')
    #  print('train_log_df')
    #  print(set(train_log_df['case:concept:name']))
    test_log_df = event_log_df.iloc[test]
    test_log_df = test_log_df.sort_values('time:timestamp')
    #  print('test_log_df')
    #  print(set(test_log_df['case:concept:name']))
    train_log = log_converter.apply(train_log_df, variant=log_converter.Variants.TO_EVENT_LOG)
    test_log = log_converter.apply(test_log_df, variant=log_converter.Variants.TO_EVENT_LOG)

    print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'discovering a process model...')
    #  print(train_log_df[['case:concept:name', 'time:timestamp', 'concept:name']])
    #  print(train_log_df.groupby('case:concept:name').size())
    #  print(set(train_log_df['case:concept:name']))
    #  print(set(train_log_df['time:timestamp']))
    #  print(set(train_log_df['concept:name']))
    net, im, fm = discover_model(train_log, miner)
    print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'process model discovered')
    score = evaluate_score(test_log, net, im, fm, metric)
    print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'score evaluated:', score)
    return score

# pretest: return True if all preliminary tests are passed, otherwise return False
def pretest(event_log_df):
    # immediately return False for some extreme cases
    # if a case only contain one activity, then it shouldn't be the case.
    n_activities_in_traces = event_log_df.groupby('case:concept:name')['concept:name'].apply(set).apply(list).apply(len).tolist()
    print('number of activities in each trace:', n_activities_in_traces)
    if (1 in n_activities_in_traces):
        print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'some traces contain only one activity. score: 0')
        return False

    # timestamp valus must include a number in it
    if (not all([bool(re.search(r'\d', value)) for value in set(event_log_df['time:timestamp'])])):
        print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'some timestamp values do not contain a number. score: 0')
        return False

    # each trace should have some common activities; otherwise there is no use to discover a model
    # for this we use cosine similarity (Jaccard can be used, but its value ranges more than 1, and it's hard to handle)
    # we don't have to calculate every pair's similarity; just randomly sample some and use average similarity for judgement
    sets = event_log_df.groupby('case:concept:name')['concept:name'].apply(list).apply(set).tolist()
    #  print('sets:', sets)
    if len(sets) > 0:
        pairs = list(all_pairs(sets, similarity_func_name="cosine", similarity_threshold=0.1))
        #  print('pairs:', pairs)
        if len(pairs) == 0:
            cosine_sim = 0
        else:
            cosine_sim = np.mean([list(pairs[i])[2] for i in range(len(pairs))])
    else:
        cosine_sim = 0

    print('cosine similarity:', cosine_sim)
    if cosine_sim <= min_cosine_sim:
        print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'few common acitivities are found in each trace. score: 0')
        return False
    # if passed all preliminary tests, then return True
    return True

def discover_model(event_log, miner):
    # default parameters are used
    if miner == 'inductive_miner':
        net, im, fm = inductive_miner.apply(event_log,
            {pm4py.algo.discovery.inductive.variants.im.algorithm.Parameters.NOISE_THRESHOLD: 0.2},
            pm4py.algo.discovery.inductive.algorithm.Variants.IM)
    elif miner == 'heuristics_miner':
        net, im, fm = heuristics_miner.apply(event_log, {
            heuristics_miner.Variants.CLASSIC.value.Parameters.DEPENDENCY_THRESH: 0.5,
            heuristics_miner.Variants.CLASSIC.value.Parameters.AND_MEASURE_THRESH: 0.65,
            heuristics_miner.Variants.CLASSIC.value.Parameters.MIN_ACT_COUNT: 1,
            heuristics_miner.Variants.CLASSIC.value.Parameters.MIN_DFG_OCCURRENCES: 1,
            heuristics_miner.Variants.CLASSIC.value.Parameters.DFG_PRE_CLEANING_NOISE_THRESH: 0.05,
            heuristics_miner.Variants.CLASSIC.value.Parameters.LOOP_LENGTH_TWO_THRESH: 2})
    return net, im, fm

def evaluate_score(event_log, net, im, fm, metric):
    if metric == 'fitness':
        score = replay_fitness_evaluator.apply(event_log, net, im, fm, 
                variant=replay_fitness_evaluator.Variants.TOKEN_BASED)['log_fitness']
    elif metric == 'precision':
        score = precision_evaluator.apply(event_log, net, im, fm, 
                variant=precision_evaluator.Variants.ETCONFORMANCE_TOKEN)
    elif metric == 'generalization':
        score = generalization_evaluator.apply(event_log, net, im, fm)
    elif metric == 'simplicity':
        score = simplicity_evaluator.apply(net)
    else:
        if metric == 'Buijs2014':
        # A paper "Quality dimensions in process discovery: The importance of fitness, 
        # precision, generalization and simplicity" proposed to calculate the following 
        # four metrics with giving 10 times more weight to replay fitness than 
        # the other three.
        # 10 x + 3x = 1 => x = 1 / 13
            weights = [10/13, 1/13, 1/13, 1/13]
        elif metric == 'average':
            weights = [0.25, 0.25, 0.25, 0.25]
        fitness = replay_fitness_evaluator.apply(event_log, net, im, fm, 
                variant=replay_fitness_evaluator.Variants.TOKEN_BASED)['log_fitness']
        precision = precision_evaluator.apply(event_log, net, im, fm, 
                variant=precision_evaluator.Variants.ETCONFORMANCE_TOKEN)
        #  fitness = replay_fitness_evaluator.apply(event_log, net, im, fm, 
                #  variant=replay_fitness_evaluator.Variants.ALIGNMENT_BASED)['log_fitness']
        #  precision = precision_evaluator.apply(event_log, net, im, fm, 
                #  variant=precision_evaluator.Variants.ALIGN_ETCONFORMANCE)
        generalization = generalization_evaluator.apply(event_log, net, im, fm)
        simplicity = simplicity_evaluator.apply(net)
        score = np.dot(weights, [fitness, precision, generalization, simplicity])
    return score

def bruteforce():
    indices = list(permutations(range(0, dim), 3))
    scores = [f(x) for x in indices]
    highest_score = np.max(scores)
    highest_score_index = np.argmax(scores)
    return [indices[highest_score_index], highest_score]

if __name__ == '__main__':
    for i in range(args.round):
        current_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        print(current_datetime, '\n--- Round {} ---'.format(i+1))
        # measure computation time
        start_time = time.time()
        if method == 'GA':
            result = CEA_MIP(f, dim, pop, gen)
        elif method == 'bruteforce':
            result = bruteforce()
    
        elapsed_time = round(time.time() - start_time)
        identified_indices = np.argsort(result[0])[0:3].tolist()
        print('Label (identified):', identified_indices)
        print('Label (correct):', labels)
        print('Highest score:', result[1])
        print('Elapsed time: {:,.1f}s'.format(elapsed_time))
        # save a result
        # extract a dataset name from its path
        tmp = re.search(r'([\w_-]+)\.csv', dataset)
        dataset_name = tmp.group(1)

        prefix = current_datetime + '-' + dataset_name + '-' + miner + '-' + method
    
        attributes = {
                'elapsed_time_in_sec': elapsed_time, 
                'dataset': dataset, 
                'miner': miner, 
                'method': method, 
                'pop': pop,
                'gen': gen,
                'metric': metric,
                'n_max_traces': n_max_traces,
                'min_cosine_sim': min_cosine_sim,
                'n_max_activities': n_max_activities,
                'evaluation_method': evaluation_method,
                'labels': labels,
                'identified labels': identified_indices
                }
        pickle.dump([attributes, result], open(os.path.join(results_path, prefix + '.pickle'), 'wb'))
