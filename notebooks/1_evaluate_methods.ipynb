{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b771b9ce-38df-4577-9e46-f42876acb184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) narrow down key column candidates with the supervised approach\n",
    "# 2) test them with the unsupervised approach to identify the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bf4144-09a5-4e0b-87a3-cd9ff0b1e7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import time\n",
    "import random \n",
    "import pickle\n",
    "import signal\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from random import sample\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "from collections import Counter\n",
    "from SetSimilaritySearch import all_pairs\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# for CV\n",
    "# process mining\n",
    "import pm4py\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "# miners\n",
    "from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n",
    "from pm4py.algo.discovery.heuristics import algorithm as heuristics_miner\n",
    "# performance metrics\n",
    "from pm4py.algo.evaluation.replay_fitness import algorithm as replay_fitness_evaluator\n",
    "from pm4py.algo.evaluation.precision import algorithm as precision_evaluator\n",
    "from pm4py.algo.evaluation.generalization import algorithm as generalization_evaluator\n",
    "from pm4py.algo.evaluation.simplicity import algorithm as simplicity_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9753d13-39cc-41c2-87a8-a6b9fe011786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_event_log(dataset_path=None, n_rows=1000):\n",
    "    try:\n",
    "        event_log = pd.read_csv(dataset_path, nrows=n_rows)\n",
    "    except:\n",
    "        return None\n",
    "    h = event_log.columns.values.tolist()\n",
    "    # check if a dataset contains a tuple of case_id, activity, and timestamp\n",
    "    if ('case:concept:name' in h) and ('concept:name' in h) and ('time:timestamp' in h):\n",
    "        print(dataset_path, 'is valid for evaluation')\n",
    "        # replace NaN with ''\n",
    "        event_log = event_log.fillna(np.nan).replace([np.nan], [''])\n",
    "        return event_log\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9be5490-d7a1-44e8-b096-8d7738b24b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_extraction: \n",
    "# input: values in a column\n",
    "# output: features\n",
    "def feature_extraction(values, round_digits=3):\n",
    "    def local_feature_extraction(value):\n",
    "        patterns = ['[a-z]', '[A-Z]', '\\d', '\\s', '[^a-zA-Z\\d\\s]']\n",
    "        f = {}\n",
    "        if len(value) == 0:\n",
    "            f['f_chars'] = 0 \n",
    "            f['f_words'] = 0\n",
    "            for p in patterns:\n",
    "                f['f_{}'.format(p)] = 0\n",
    "        else:\n",
    "            # length: length of a value\n",
    "            f['f_chars'] = len(value)\n",
    "            # words: number of words in a value\n",
    "            f['f_words'] = len(re.findall(r'\\w+', value))\n",
    "            # The following code find the frequency of each pattern in patterns in a value\n",
    "            for p in patterns:\n",
    "                f['f_{}'.format(p)] = round(len(re.findall(p, value)) / len(value), round_digits)\n",
    "        return f\n",
    "\n",
    "    # set type of values string\n",
    "    values = values.astype(str)\n",
    "    # local features\n",
    "    f_local = [local_feature_extraction(value) for value in values]\n",
    "    # convert it into a DF to easily calculate mean of each feature\n",
    "    f = pd.DataFrame.from_dict(f_local)\n",
    "    f = f.apply(np.mean, axis=0)\n",
    "    # global features\n",
    "    # count the occurence of each value in values\n",
    "    counts = Counter(values).values()\n",
    "    if len(counts) > 1:\n",
    "        # f_ratio_unique_values: how much unique values are involved\n",
    "        f['f_ratio_unique_values'] = round(len(set(values)) / len(values), round_digits)\n",
    "        # f_mean_unique_values: mean value of number of appearance of each value\n",
    "        f['f_mean_unique_values'] = round(statistics.mean(counts), round_digits)\n",
    "    else:\n",
    "        f['f_ratio_unique_values'] = 1\n",
    "        f['f_mean_unique_values'] = 1\n",
    "    return f.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbaee99-353f-4d10-90bf-f8dce54a0e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relabel: if a label is a key label, keep it. Otherwise, relabel it as 'other'\n",
    "def relabel(labels, target_class=None):\n",
    "    return np.array([label if label == target_class else 'other' for label in labels])\n",
    "\n",
    "# flatten: flatten an array of arrays\n",
    "# https://stackoverflow.com/a/952952/7184459\n",
    "def flatten(x):\n",
    "    return [x_elem_elem for x_elem in x for x_elem_elem in x_elem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd99ad9-cb33-4488-8b39-265f43eb314c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_key_column_cands(features, target_class, train_index, test_index, \\\n",
    "                         n_others=1, n_cands=2, n_estimators=100):\n",
    "    def make_train_df(feature=None):\n",
    "        df = feature.iloc[:, feature.columns != target_class]\\\n",
    "            .sample(n=n_others, axis=1)\n",
    "        df[target_class] = feature[target_class]\n",
    "        return df\n",
    "\n",
    "    # make a training dataset\n",
    "    tmp_df = [make_train_df(feature=features[i]) for i in train_index]\n",
    "    y = np.array([np.array(relabel(labels=e.columns.values, target_class=target_class)) \\\n",
    "                            for e in tmp_df], dtype=object)\n",
    "    X = np.array([np.transpose(e).values for e in tmp_df], dtype=object)\n",
    "    X_train = flatten(X)\n",
    "    y_train = flatten(y)\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # make a test dataset\n",
    "    tmp_df = [features[i] for i in test_index]\n",
    "    y = np.array([np.array(relabel(labels=e.columns.values, target_class=target_class)) \\\n",
    "                            for e in tmp_df], dtype=object)\n",
    "    X = np.array([np.transpose(e).values for e in tmp_df], dtype=object)\n",
    "    X_test = flatten(X)\n",
    "    y_test = flatten(y)\n",
    "    # evaluate the classifier\n",
    "    # only return the highest n_cands candidates\n",
    "    probs = clf.predict_proba(X_test)\n",
    "    # the probs contain two probabilities (for binary classficiation), but we need to check\n",
    "    # which element refers to the target class's probability\n",
    "    target_class_index = np.where(clf.classes_ == target_class)\n",
    "    # print('target_class_index', target_class_index, 'of', clf.classes_)\n",
    "    probs = [probs[i][target_class_index] for i in range(len(probs))]\n",
    "    # print(probs)\n",
    "    top_probs = sorted(probs)[-n_cands:]\n",
    "    # a candidate must be identified as a target key (i.e. judge if a prob > 0.5)\n",
    "    top_probs = [prob for prob in top_probs if prob > 0.5]\n",
    "    cands = np.where([p in top_probs for p in probs])[0].tolist()\n",
    "    # print(cands)\n",
    "    return cands\n",
    "    \n",
    "    # The previous idea just returns every column index where its class is identified as a target key\n",
    "    # y_predicted = clf.predict(X_test)\n",
    "    # print(classification_report(y_test, y_predicted))\n",
    "    # return the indices of timestamp candidtes\n",
    "    # return np.where([value == target_class for value in y_predicted])[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b582450e-e553-407a-9814-b32c9fe6b8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_check(d=None, key_indices=None, n_samples=10, min_cosine_sim=0.5):\n",
    "    event_log_df = d.iloc[:, key_indices]\n",
    "    event_log_df.columns.values[0] = 'case:concept:name'\n",
    "    event_log_df.columns.values[1] = 'time:timestamp'\n",
    "    event_log_df.columns.values[2] = 'concept:name'\n",
    "    # get the number of cases\n",
    "    n_cases = len(set(event_log_df['case:concept:name']))\n",
    "    if n_cases > n_samples:\n",
    "        # sample n_sample cases\n",
    "        sampled_case_ids = sample(list(set(event_log_df['case:concept:name'])), n_samples)\n",
    "        event_log_df = event_log_df[event_log_df['case:concept:name'].isin(sampled_case_ids)]\n",
    "    \n",
    "    # get the sequence of activities by case_ids\n",
    "    sets = event_log_df.groupby('case:concept:name')['concept:name']\\\n",
    "        .apply(list).apply(set).tolist()\n",
    "    if len(sets) > 0:\n",
    "        pairs = list(all_pairs(sets, similarity_func_name=\"cosine\", similarity_threshold=0.1))\n",
    "        #  print('pairs:', pairs)\n",
    "        if len(pairs) == 0:\n",
    "            cosine_sim = 0\n",
    "        else:\n",
    "            cosine_sim = np.mean([list(pairs[i])[2] for i in range(len(pairs))])\n",
    "    else:\n",
    "        cosine_sim = 0\n",
    "\n",
    "    # print('cosine similarity:', cosine_sim)\n",
    "    if cosine_sim <= min_cosine_sim:\n",
    "        return False\n",
    "    else:\n",
    "        # if passed all preliminary tests, then return True\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9438a82e-93f1-4a8b-ba61-6233d6a86971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_score(d=None, key_indices=None, miner='inductive_miner', metric='generalization', n_splits=2,\n",
    "                  model_discovery_timeout_in_sec=5, evaluation_timeout_in_sec=60):\n",
    "    def discover_model(event_log, miner):\n",
    "        # default parameters are used\n",
    "        if miner == 'inductive_miner':\n",
    "            net, im, fm = inductive_miner.apply(event_log,\n",
    "                {pm4py.algo.discovery.inductive.variants.im.algorithm.Parameters.NOISE_THRESHOLD: 0.2},\n",
    "                pm4py.algo.discovery.inductive.algorithm.Variants.IM)\n",
    "        elif miner == 'heuristics_miner':\n",
    "            net, im, fm = heuristics_miner.apply(event_log, {\n",
    "                heuristics_miner.Variants.CLASSIC.value.Parameters.DEPENDENCY_THRESH: 0.5,\n",
    "                heuristics_miner.Variants.CLASSIC.value.Parameters.AND_MEASURE_THRESH: 0.65,\n",
    "                heuristics_miner.Variants.CLASSIC.value.Parameters.MIN_ACT_COUNT: 1,\n",
    "                heuristics_miner.Variants.CLASSIC.value.Parameters.MIN_DFG_OCCURRENCES: 1,\n",
    "                heuristics_miner.Variants.CLASSIC.value.Parameters.DFG_PRE_CLEANING_NOISE_THRESH: 0.05,\n",
    "                heuristics_miner.Variants.CLASSIC.value.Parameters.LOOP_LENGTH_TWO_THRESH: 2})\n",
    "        return net, im, fm\n",
    "\n",
    "    def evaluate_score(event_log, net, im, fm, metric):\n",
    "        if metric == 'fitness':\n",
    "            score = replay_fitness_evaluator.apply(event_log, net, im, fm, \n",
    "                    variant=replay_fitness_evaluator.Variants.TOKEN_BASED)['log_fitness']\n",
    "        elif metric == 'precision':\n",
    "            score = precision_evaluator.apply(event_log, net, im, fm, \n",
    "                    variant=precision_evaluator.Variants.ETCONFORMANCE_TOKEN)\n",
    "        elif metric == 'generalization':\n",
    "            score = generalization_evaluator.apply(event_log, net, im, fm)\n",
    "        elif metric == 'simplicity':\n",
    "            score = simplicity_evaluator.apply(net)\n",
    "        else:\n",
    "            if metric == 'Buijs2014':\n",
    "            # A paper \"Quality dimensions in process discovery: The importance of fitness, \n",
    "            # precision, generalization and simplicity\" proposed to calculate the following \n",
    "            # four metrics with giving 10 times more weight to replay fitness than \n",
    "            # the other three.\n",
    "            # 10 x + 3x = 1 => x = 1 / 13\n",
    "                weights = [10/13, 1/13, 1/13, 1/13]\n",
    "            elif metric == 'average':\n",
    "                weights = [0.25, 0.25, 0.25, 0.25]\n",
    "            fitness = replay_fitness_evaluator.apply(event_log, net, im, fm, \n",
    "                    variant=replay_fitness_evaluator.Variants.TOKEN_BASED)['log_fitness']\n",
    "            precision = precision_evaluator.apply(event_log, net, im, fm, \n",
    "                    variant=precision_evaluator.Variants.ETCONFORMANCE_TOKEN)\n",
    "            #  fitness = replay_fitness_evaluator.apply(event_log, net, im, fm, \n",
    "                    #  variant=replay_fitness_evaluator.Variants.ALIGNMENT_BASED)['log_fitness']\n",
    "            #  precision = precision_evaluator.apply(event_log, net, im, fm, \n",
    "                    #  variant=precision_evaluator.Variants.ALIGN_ETCONFORMANCE)\n",
    "            generalization = generalization_evaluator.apply(event_log, net, im, fm)\n",
    "            simplicity = simplicity_evaluator.apply(net)\n",
    "            score = np.dot(weights, [fitness, precision, generalization, simplicity])\n",
    "        return score\n",
    "    \n",
    "    def timeout_handler(signum, frame):\n",
    "        raise Exception('timeout')\n",
    "        \n",
    "    def cross_validation(event_log_df, train, test):\n",
    "        # dataset need be sorted by timestamp (https://pm4py.fit.fraunhofer.de/documentation#item-import-csv)\n",
    "        #  print(\"%s %s\" % (train, test))\n",
    "        train_log_df = event_log_df.iloc[train]\n",
    "        try:\n",
    "            train_log_df = train_log_df.sort_values('time:timestamp')\n",
    "        except:\n",
    "            print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'the identified timestamp seems wrong', 0)\n",
    "            return 0\n",
    "            \n",
    "        test_log_df = event_log_df.iloc[test]\n",
    "        try:\n",
    "            test_log_df = test_log_df.sort_values('time:timestamp')\n",
    "        except:\n",
    "            print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'the identified timestamp seems wrong', 0)\n",
    "            return 0\n",
    "        \n",
    "        train_log = log_converter.apply(train_log_df, \\\n",
    "                                        variant=log_converter.Variants.TO_EVENT_LOG)\n",
    "        test_log = log_converter.apply(test_log_df, \\\n",
    "                                       variant=log_converter.Variants.TO_EVENT_LOG)\n",
    "    \n",
    "        print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), \\\n",
    "              'discovering a process model...')\n",
    "        \n",
    "        # discover_model may take time. hence, we set a timeout for it\n",
    "        # https://stackoverflow.com/a/494273/7184459\n",
    "        signal.signal(signal.SIGALRM, timeout_handler)\n",
    "        signal.alarm(model_discovery_timeout_in_sec)\n",
    "        try:\n",
    "            net, im, fm = discover_model(train_log, miner)\n",
    "        except Exception:\n",
    "            print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'gave up discovering a model')\n",
    "            print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'score evaluated:', 0)\n",
    "            return 0\n",
    "        print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'process model discovered')\n",
    "        # reset a timer when a model discovery finishes before the deadline\n",
    "        signal.alarm(0)\n",
    "        # set another timer for evaluation\n",
    "        signal.signal(signal.SIGALRM, timeout_handler)\n",
    "        signal.alarm(evaluation_timeout_in_sec)\n",
    "        try:\n",
    "            score = evaluate_score(test_log, net, im, fm, metric)\n",
    "        except Exception:\n",
    "            print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'gave up evaluating a model')\n",
    "            print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'score evaluated:', 0)\n",
    "            return 0\n",
    "        signal.alarm(0)\n",
    "        print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'score evaluated:', score)\n",
    "        return score\n",
    "    \n",
    "    # assume the columns as case_id, timestamp, and activity from left\n",
    "    event_log_df = d.iloc[:, key_indices]\n",
    "    event_log_df.columns = ['case:concept:name', 'time:timestamp', 'concept:name']\n",
    "    # print(event_log_df.head())\n",
    "    # evaluate the goodness with CV\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    X = list(event_log_df.index)\n",
    "    groups = list(event_log_df['case:concept:name'])\n",
    "    if len(groups) < n_splits:\n",
    "        print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'the case_id seems wrong')\n",
    "        return 0\n",
    "    scores = [cross_validation(event_log_df, train, test) \\\n",
    "          for train, test in gkf.split(X, groups=groups)]\n",
    "    score = round(np.mean(scores), ndigits=3)\n",
    "    print('final score:', score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2e17ad-cbcc-48df-b4a5-1edaace0ca5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def narrow_down_cands_and_measure_scores(dataset=None, verbose=False, \\\n",
    "                                         miner='inductive_miner', metric='Buijs2014', n_splits=2):\n",
    "    cands = get_combinations_to_try(dataset=dataset, verbose=verbose)\n",
    "    # after pre-filtering the candidates, try every combination to process discovery\n",
    "    # the intuition behind it is that the correct key column combination would result in\n",
    "    # a good score (e.g. fitness, precision, generalizability, simplicity)\n",
    "    scores = [{'indices': key_indices, \\\n",
    "               'score': measure_score(dataset, key_indices, miner=miner, \\\n",
    "                                      metric=metric, n_splits=n_splits)} for key_indices in cands]\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e2c453-e628-4361-a19f-32bd512c7c87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasets = sorted(glob.glob('../datasets/BPIC*.csv', recursive=False))\n",
    "event_logs = [load_event_log(d, n_rows=1000) for d in datasets]\n",
    "# remove unqualified event logs\n",
    "valid_indices = [e is not None for e in event_logs]\n",
    "# need to update datasets too\n",
    "datasets = [datasets[i] for i in range(len(datasets)) if valid_indices[i]]\n",
    "event_logs = [e for e in event_logs if e is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b552f180-0689-4bb6-9baf-e134458ab9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [e.apply(lambda x: feature_extraction(x), axis=0) for e in event_logs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119cf818-a919-46b9-95c8-c9891fc1e361",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_splits = len(event_logs)\n",
    "n_estimators = 100\n",
    "n_cands_filtering = [1, 2]\n",
    "miners = ['inductive_miner', 'heuristics_miner']\n",
    "metrics = ['Buijs2014', 'fitness', 'precision', 'generalization', 'simplicity']\n",
    "maximum_trials = 10\n",
    "n_repeats = 10\n",
    "results = []\n",
    "for n in range(n_repeats):\n",
    "    # performance evaluation with leave-one-out (use one for test and the others for training)\n",
    "    for i in range(len(features)):\n",
    "        test_index = i\n",
    "        train_index = list(set(range(len(features))) - set([test_index]))\n",
    "        # print('TRAIN:', train_index)\n",
    "        print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'evaluating:', datasets[test_index])\n",
    "        \n",
    "        for i_n_cands in n_cands_filtering:\n",
    "            # narrow down key column candidates with a supervised approach\n",
    "            # build a binary classifier for each key column\n",
    "            # try multiple times to find candidates\n",
    "            start_time = time.time()\n",
    "            found_cands = False\n",
    "            for i_trial in range(maximum_trials):\n",
    "                case_id_cand = get_key_column_cands(features, 'case:concept:name', \n",
    "                                                    train_index, [test_index], 1, i_n_cands)\n",
    "                timestamp_cand = get_key_column_cands(features, 'time:timestamp', \n",
    "                                                      train_index, [test_index], 1, i_n_cands)\n",
    "                activity_cand = get_key_column_cands(features, 'concept:name', \n",
    "                                                     train_index, [test_index], 1, i_n_cands)\n",
    "                print('case_id_cand:', case_id_cand)\n",
    "                print('timestamp_cand:', timestamp_cand)\n",
    "                print('activity_cand:', activity_cand)\n",
    "                if all([len(timestamp_cand) > 0, len(case_id_cand) > 0, len(activity_cand) > 0]):\n",
    "                    found_cands = True\n",
    "                    break\n",
    "            if not found_cands:\n",
    "                print('Could not find key column candidates. Skip this test.')\n",
    "                break\n",
    "            \n",
    "            if all([len(case_id_cand) == 1, len(timestamp_cand) == 1, len(activity_cand) == 1]):\n",
    "                combinations_to_try = [[case_id_cand[0], timestamp_cand[0], activity_cand[0]]]\n",
    "            else:\n",
    "                key_index_cand = [list(t) for t in product(case_id_cand, timestamp_cand, activity_cand) if len(set(t)) >= 3]\n",
    "                # as similar activity sequences have to be observed by multiple cases (otherwise we cannot identify a process flow)\n",
    "                # hence we rule out the cases where each sequence of activities do not share a common pattern\n",
    "                # in this regrad, we use cosine similarity to measure the overlaps between activities\n",
    "                tmp = [similarity_check(event_logs[test_index], key_indices, n_samples=10, min_cosine_sim=0.3) \\\n",
    "                    for key_indices in key_index_cand]\n",
    "                combinations_to_try = np.array(key_index_cand)[np.array(tmp)].tolist()\n",
    "            time_cand_selection = time.time() - start_time\n",
    "            \n",
    "            h = event_logs[test_index].columns.values.tolist()\n",
    "            correct_indices = [\n",
    "                h.index('case:concept:name'),\n",
    "                h.index('time:timestamp'),\n",
    "                h.index('concept:name')\n",
    "            ]\n",
    "            \n",
    "            # if only one candidate is identified, we can skip the second stage\n",
    "            if all([len(case_id_cand) == 1, len(timestamp_cand) == 1, len(activity_cand) == 1]):\n",
    "                results.append({\n",
    "                    'dataset': datasets[test_index], \n",
    "                    'time_cand_selection': time_cand_selection, \n",
    "                    'time_score_eval': 0, \n",
    "                    'n_timestamp_cand': 1, \n",
    "                    'n_case_id_cand': 1,\n",
    "                    'n_activity_cand': 1,\n",
    "                    'n_top_cands': i_n_cands,\n",
    "                    'n_cands': 1,\n",
    "                    'scores': [],\n",
    "                    'correct_indices': correct_indices, \n",
    "                    'cands': combinations_to_try, \n",
    "                    'miner': None,\n",
    "                    'metric': None, \n",
    "                    'n_RF_estimators': None\n",
    "                })\n",
    "            else:    \n",
    "                for i_miner in miners:\n",
    "                    for i_metric in metrics:\n",
    "                        print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'miner:', i_miner, 'metric:', i_metric)\n",
    "                        start_time = time.time()\n",
    "                        scores = [measure_score(event_logs[test_index], key_indices, miner=i_miner, \n",
    "                                                metric=i_metric, n_splits=2) for key_indices in combinations_to_try]\n",
    "                        time_score_eval = time.time() - start_time\n",
    "                        \n",
    "                        results.append({\n",
    "                            'dataset': datasets[test_index], \n",
    "                            'time_cand_selection': time_cand_selection, \n",
    "                            'time_score_eval': time_score_eval, \n",
    "                            'n_timestamp_cand': len(timestamp_cand), \n",
    "                            'n_case_id_cand': len(case_id_cand), \n",
    "                            'n_activity_cand': len(activity_cand), \n",
    "                            'n_top_cands': i_n_cands,\n",
    "                            'n_cands': len(combinations_to_try), \n",
    "                            'scores': scores, \n",
    "                            'correct_indices': correct_indices, \n",
    "                            'cands': combinations_to_try, \n",
    "                            'miner': i_miner, \n",
    "                            'metric': i_metric, \n",
    "                            'n_RF_estimators': n_estimators\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cc0d5d-a9ee-434b-b3dc-512bc93eb64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../results/results.pickle', 'wb') as output_file:\n",
    "    pickle.dump(results, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b64f0af-d7c5-43de-ae6b-30205a460eff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd0bccb",
   "metadata": {},
   "source": [
    "# Evaluate the first stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93909270",
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2719e6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = len(event_logs)\n",
    "n_estimators = 100\n",
    "n_cands_filtering = range(1, 5)\n",
    "maximum_trials = 10\n",
    "n_repeats = 10\n",
    "results = []\n",
    "for n in range(n_repeats):\n",
    "    # performance evaluation with leave-one-out (use one for test and the others for training)\n",
    "    for i in range(len(features)):\n",
    "        test_index = i\n",
    "        train_index = list(set(range(len(features))) - set([test_index]))\n",
    "        # print('TRAIN:', train_index)\n",
    "        print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'evaluating:', datasets[test_index])\n",
    "        \n",
    "        for i_n_cands in n_cands_filtering:\n",
    "            # narrow down key column candidates with a supervised approach\n",
    "            # build a binary classifier for each key column\n",
    "            # try multiple times to find candidates\n",
    "            start_time = time.time()\n",
    "            found_cands = False\n",
    "            print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'evaluating k', i_n_cands)\n",
    "            for i_trial in range(maximum_trials):\n",
    "                case_id_cand = get_key_column_cands(features, 'case:concept:name', \n",
    "                                                    train_index, [test_index], 1, i_n_cands)\n",
    "                timestamp_cand = get_key_column_cands(features, 'time:timestamp', \n",
    "                                                      train_index, [test_index], 1, i_n_cands)\n",
    "                activity_cand = get_key_column_cands(features, 'concept:name', \n",
    "                                                     train_index, [test_index], 1, i_n_cands)\n",
    "                print('case_id_cand:', case_id_cand)\n",
    "                print('timestamp_cand:', timestamp_cand)\n",
    "                print('activity_cand:', activity_cand)\n",
    "                if all([len(timestamp_cand) > 0, len(case_id_cand) > 0, len(activity_cand) > 0]):\n",
    "                    found_cands = True\n",
    "                    break\n",
    "            if not found_cands:\n",
    "                print('Could not find key column candidates. Skip this test.')\n",
    "                break\n",
    "            \n",
    "            if all([len(case_id_cand) == 1, len(timestamp_cand) == 1, len(activity_cand) == 1]):\n",
    "                combinations_to_try = [[case_id_cand[0], timestamp_cand[0], activity_cand[0]]]\n",
    "            else:\n",
    "                key_index_cand = [list(t) for t in product(case_id_cand, timestamp_cand, activity_cand) if len(set(t)) >= 3]\n",
    "                # as similar activity sequences have to be observed by multiple cases (otherwise we cannot identify a process flow)\n",
    "                # hence we rule out the cases where each sequence of activities do not share a common pattern\n",
    "                # in this regrad, we use cosine similarity to measure the overlaps between activities\n",
    "                tmp = [similarity_check(event_logs[test_index], key_indices, n_samples=10, min_cosine_sim=0.3) \\\n",
    "                    for key_indices in key_index_cand]\n",
    "                if len(tmp) == 0 or len(key_index_cand) == 0:\n",
    "                    combinations_to_try = []\n",
    "                else:\n",
    "                    combinations_to_try = np.array(key_index_cand)[np.array(tmp)].tolist()\n",
    "            time_cand_selection = time.time() - start_time\n",
    "            \n",
    "            h = event_logs[test_index].columns.values.tolist()\n",
    "            correct_indices = [\n",
    "                h.index('case:concept:name'),\n",
    "                h.index('time:timestamp'),\n",
    "                h.index('concept:name')\n",
    "            ]\n",
    "            \n",
    "            # Immediately return the result after the first stage\n",
    "            results.append({\n",
    "                'dataset': datasets[test_index], \n",
    "                'time_cand_selection': time_cand_selection, \n",
    "                'n_timestamp_cand': len(timestamp_cand), \n",
    "                'n_case_id_cand': len(case_id_cand), \n",
    "                'n_activity_cand': len(activity_cand), \n",
    "                'n_top_cands': i_n_cands,\n",
    "                'n_cands': len(combinations_to_try), \n",
    "                'correct_indices': correct_indices, \n",
    "                'cands': combinations_to_try, \n",
    "                'n_RF_estimators': n_estimators\n",
    "            })\n",
    "\n",
    "with open(r'../results/results_first_stage.pickle', 'wb') as output_file:\n",
    "    pickle.dump(results, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
